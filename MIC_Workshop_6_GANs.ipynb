{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIC Workshop 6: GANs",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon-lin/ML-notebooks/blob/master/MIC_Workshop_6_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mdhxnn2Zh4qO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MIC Workshop 6: Generative Adversarial Networks\n"
      ]
    },
    {
      "metadata": {
        "id": "3dsXnHq_ivZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "Hope you're enjoying the Deep Learning workshops so far! For these workshops, we'll typically use Google Colab, an online coding environment. This is so that we don't have to worry about installing all of the libraries on everyone's different computers. \n",
        "\n",
        "_____\n",
        "You're now working in a Notebook. Notebooks have **cells**, each of which can be run by hitting Shift+Enter. Try it on the cell below!\n",
        "\n",
        "_You will see the output of the particular cell right below it_"
      ]
    },
    {
      "metadata": {
        "id": "r_kCRR_SlEHd",
        "colab_type": "code",
        "outputId": "e28462b6-8f8d-4c82-9702-eb619541abe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Notebooks are so much fun!!!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Notebooks are so much fun!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rq3WNwgDlIsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In general, notebooks are a very suitable tool for machine learning/data science. We would also recommend trying [Jupyter Notebook](https://jupyter.org/install) if you haven't already\n",
        "\n",
        "_For later workshops, we might opt out for a more involved environment like Docker in case we want to do anything fancier_\n",
        "____\n",
        "## Installing PyTorch\n",
        "Don't worry too much about the contents of this cell. It basically just installs the right packages for you to run PyTorch code\n",
        "\n",
        "If this cell is causing problems for you (like `tcmalloc`,  make sure you click \"connect to Hosted runtime\" from the dropdown menu in the top right)\n"
      ]
    },
    {
      "metadata": {
        "id": "O6rJyKpzkMOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "440728e3-adcf-4124-d7f4-9542e0508ebf"
      },
      "cell_type": "code",
      "source": [
        "# Installing pytorch, don't worry about the code in this cell. \n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = 'cpu' #cuda_output[0] if exists('/dev/nvidia0')\n",
        "\n",
        "!pip3 install torchvision\n",
        "!pip install unidecode\n",
        "!pip install tensorboardX\n",
        "\n",
        "#!pip install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.1.post2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 7.4MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.23\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.2)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.9.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yYw_ugeGkNDY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from IPython import display\n",
        "import tqdm\n",
        "import time\n",
        "from tensorboardX import SummaryWriter\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "use_cuda = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ScqXwtnAl5kE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is notorioulsy difficult to get a stable GAN. For that reason, let's look at a very simple example today."
      ]
    },
    {
      "metadata": {
        "id": "088I6pyUOtqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Let's try to approximate a Gaussian with a mean and variance of our choice."
      ]
    },
    {
      "metadata": {
        "id": "bv7vf5xpPGUG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Without loss of generality, let's say that $\\mu = 5$  and $\\sigma^2 = 16$.**"
      ]
    },
    {
      "metadata": {
        "id": "djgBVhccPbgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*A note on probability:*\n",
        "\n",
        "If we had a source of randomness that provided us with a \"truly random\" number $x$ in the interval $[0,1]$, we could just take $\\sigma\\Phi^{-1}(x) + \\mu$, where $\\Phi(\\cdot)$ is the CDF of the standard normal. \n",
        "\n",
        "In other words, we wouldn't necessarily need fancy machinery for this. Nevertheless, let's try out how well the GANs we just learned about perform."
      ]
    },
    {
      "metadata": {
        "id": "mHzM1rkQRFAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instead of taking that approach, let's make our fancy GAN model. We would need a \n",
        "\n",
        "```\n",
        "Generator\n",
        "```\n",
        "and a\n",
        "```\n",
        "Discriminator.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-mNG5y2uRubv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Let's first make sure we have access to an actual Gaussian.**"
      ]
    },
    {
      "metadata": {
        "id": "ccbskLj7luy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9PtoEx7REcG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can try feeding our **Generator** with values sampled from a uniform distribution on $[0,1]$. Just like this: "
      ]
    },
    {
      "metadata": {
        "id": "ArxO3tyTS3gV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_generator_input_sampler():\n",
        "    return lambda m, n: torch.rand(m, n) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmDurUW6S4MG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a583b9e1-3a70-477c-cbec-0721487dd2cc"
      },
      "cell_type": "code",
      "source": [
        "get_generator_input_sampler()(5,3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6124, 0.4040, 0.3029],\n",
              "        [0.7193, 0.3918, 0.6619],\n",
              "        [0.9301, 0.2627, 0.0198],\n",
              "        [0.7621, 0.0982, 0.4237],\n",
              "        [0.3819, 0.8969, 0.4771]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "dvFI5-PFS6oU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discussion question:\n",
        "What would then be the job of the generator (together with the discriminator)? What function are they trying to learn?"
      ]
    },
    {
      "metadata": {
        "id": "WimV5wKuTXq9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Suppose we have the following two networks. Feel free to modify them in any way you want."
      ]
    },
    {
      "metadata": {
        "id": "VUc_FAvqTdlF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        self.f = f\n",
        "        self.hidden_f = torch.tanh # change?\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = self.hidden_f(x)\n",
        "        x = self.map2(x)\n",
        "        x = self.hidden_f(x)\n",
        "        x = self.map3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00nbu7SCTeLf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        self.f = f\n",
        "        self.hidden_f = torch.tanh # change?\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_f(self.map1(x))\n",
        "        x = self.hidden_f(self.map2(x))\n",
        "        return self.f(self.map3(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHmWZnyiQAUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With that, we are ready to go and train our model."
      ]
    },
    {
      "metadata": {
        "id": "byDSW9jzTwGe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Model parameters\n",
        "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
        "    g_output_size = 1     # Size of generated output vector\n",
        "    d_input_size = 500    # Minibatch size - cardinality of distributions\n",
        "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
        "    minibatch_size = d_input_size\n",
        "\n",
        "    num_epochs = 2000  \n",
        "    print_interval = 100\n",
        "\n",
        "    # Which of the above can we try to tweak?\n",
        "    g_hidden_size = 30     # Generator complexity\n",
        "    d_hidden_size = 30    # Discriminator complexity\n",
        "\n",
        "    d_learning_rate = 1e-3\n",
        "    g_learning_rate = 1e-3\n",
        "    sgd_momentum = 0.9\n",
        "    \n",
        "    d_steps = 10\n",
        "    g_steps = 30\n",
        "\n",
        "    \n",
        "    dfe, dre, ge = 0, 0, 0\n",
        "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
        "\n",
        "    # Should we change these?\n",
        "    discriminator_activation_function = torch.sigmoid\n",
        "    generator_activation_function = torch.tanh\n",
        "\n",
        "#     -------------------\n",
        "#     Nothing interesting happening here\n",
        "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "    gi_sampler = get_generator_input_sampler()\n",
        "    G = Generator(input_size=g_input_size,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size=g_output_size,\n",
        "                  f=generator_activation_function)\n",
        "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
        "                      hidden_size=d_hidden_size,\n",
        "                      output_size=d_output_size,\n",
        "                      f=discriminator_activation_function)\n",
        "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
        "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
        "#     -------------------\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for d_index in range(d_steps):\n",
        "            # 1. Train D on real+fake\n",
        "            D.zero_grad()\n",
        "\n",
        "            #  1A: Train D on real\n",
        "            d_real_data = Variable(d_sampler(d_input_size))\n",
        "            d_real_decision = D(preprocess(d_real_data))\n",
        "            d_real_error = criterion(d_real_decision, Variable(torch.ones([1,1])))  # ones = true\n",
        "            d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "            #  1B: Train D on fake\n",
        "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
        "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1,1])))  # zeros = fake\n",
        "            d_fake_error.backward()\n",
        "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
        "\n",
        "        for g_index in range(g_steps):\n",
        "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "            G.zero_grad()\n",
        "\n",
        "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "            g_fake_data = G(gen_input)\n",
        "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
        "            g_error = criterion(dg_fake_decision, Variable(torch.ones([1,1])))  # Train G to pretend it's genuine\n",
        "\n",
        "            g_error.backward()\n",
        "            g_optimizer.step()  # Only optimizes G's parameters\n",
        "            ge = extract(g_error)[0]\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
        "                  (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
        "\n",
        "    print(\"Plotting the generated distribution...\")\n",
        "    values = extract(g_fake_data)\n",
        "    print(\" Values: %s\" % (str(values)))\n",
        "    plt.hist(values, bins=50)\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Histogram of Generated Distribution')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVi0gH9WXKBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# helper functions, don't worry about them\n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "\n",
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQHtGLBmZOfg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_moments(d):\n",
        "    # Return the first 4 moments of the data provided\n",
        "    mean = torch.mean(d)\n",
        "    diffs = d - mean\n",
        "    var = torch.mean(torch.pow(diffs, 2.0))\n",
        "    std = torch.pow(var, 0.5)\n",
        "    zscores = diffs / std\n",
        "    skews = torch.mean(torch.pow(zscores, 3.0)) # should be 0\n",
        "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # should be 0\n",
        "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
        "    return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2jUc60GZPx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
        "    mean = torch.mean(data.data, 1, keepdim=True)\n",
        "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
        "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
        "    if remove_raw_data:\n",
        "        return torch.cat([diffs], 1)\n",
        "    else:\n",
        "        return torch.cat([data, diffs], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jn3-LighUJmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
        "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
        "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
        "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efSnxEftT38M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_mean = 4\n",
        "data_stddev = 1.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "poGfYLNdT1Rq",
        "colab_type": "code",
        "outputId": "8badbde5-4291-42d4-df10-fbead9aade95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: D (0.5926645398139954 real_err, 0.5785809755325317 fake_err) G (0.811711311340332 err); Real Dist ([4.020793837428093, 1.2057239547654734]),  Fake Dist ([0.10046711359918117, 0.03323015332170268]) \n",
            "Epoch 100: D (0.671867847442627 real_err, 0.6368222236633301 fake_err) G (0.7088874578475952 err); Real Dist ([3.983368607908487, 1.2783110133968822]),  Fake Dist ([6.642817407608033, 1.7451584440179528]) \n",
            "Epoch 200: D (0.7550183534622192 real_err, 0.7303162217140198 fake_err) G (0.6406751871109009 err); Real Dist ([3.928498724579811, 1.1893773463249993]),  Fake Dist ([3.621812135219574, 1.6435753700509457]) \n",
            "Epoch 300: D (0.6464150547981262 real_err, 0.702130138874054 fake_err) G (0.6871411204338074 err); Real Dist ([4.003934324860573, 1.260248641146983]),  Fake Dist ([4.0868424091339115, 1.4377602182681655]) \n",
            "Epoch 400: D (0.6771487593650818 real_err, 0.6674289703369141 fake_err) G (0.6904081702232361 err); Real Dist ([3.9244540418386458, 1.2040759169827442]),  Fake Dist ([3.985192851781845, 1.1234554460303106]) \n",
            "Epoch 500: D (0.7047691941261292 real_err, 0.6957190632820129 fake_err) G (0.6879661083221436 err); Real Dist ([3.9669762518405913, 1.2747549133782623]),  Fake Dist ([3.981633157968521, 1.2301591568557921]) \n",
            "Epoch 600: D (0.703015923500061 real_err, 0.6950687766075134 fake_err) G (0.6914781928062439 err); Real Dist ([4.015047501146793, 1.286743262478253]),  Fake Dist ([3.9290331728458403, 1.247472732883074]) \n",
            "Epoch 700: D (0.7116751074790955 real_err, 0.6903201937675476 fake_err) G (0.697974681854248 err); Real Dist ([3.9754243349432947, 1.1669105865280105]),  Fake Dist ([3.9090657012462615, 1.1958603493808853]) \n",
            "Epoch 800: D (0.6917799711227417 real_err, 0.6963889598846436 fake_err) G (0.6911527514457703 err); Real Dist ([4.047693645339459, 1.2654750775148802]),  Fake Dist ([4.006905873537064, 1.2561795126651683]) \n",
            "Epoch 900: D (0.6855787038803101 real_err, 0.6687415242195129 fake_err) G (0.6972503066062927 err); Real Dist ([4.036011714577675, 1.2179627492518934]),  Fake Dist ([4.030985701322556, 1.150927759447634]) \n",
            "Epoch 1000: D (0.7036085724830627 real_err, 0.6926454305648804 fake_err) G (0.693973958492279 err); Real Dist ([3.9379211703538894, 1.2328573914323842]),  Fake Dist ([4.117552254915237, 1.3230947686214647]) \n",
            "Epoch 1100: D (0.6892030835151672 real_err, 0.6937184929847717 fake_err) G (0.6935422420501709 err); Real Dist ([4.038906641662121, 1.2452022824365454]),  Fake Dist ([3.9470506863594057, 1.1837632115092676]) \n",
            "Epoch 1200: D (0.6932897567749023 real_err, 0.6888341903686523 fake_err) G (0.692662239074707 err); Real Dist ([3.9767797917723655, 1.262038652627858]),  Fake Dist ([3.9333877828121184, 1.2757552404342516]) \n",
            "Epoch 1300: D (0.6968416571617126 real_err, 0.6913051605224609 fake_err) G (0.6972255706787109 err); Real Dist ([3.9507884757518767, 1.2246422873046074]),  Fake Dist ([4.010757848978042, 1.2503466625534043]) \n",
            "Epoch 1400: D (0.6947935819625854 real_err, 0.6892642378807068 fake_err) G (0.6910265684127808 err); Real Dist ([3.9458339549303054, 1.2315948536469452]),  Fake Dist ([4.1621650695800785, 1.3538279875478547]) \n",
            "Epoch 1500: D (0.6850043535232544 real_err, 0.692986011505127 fake_err) G (0.6928439736366272 err); Real Dist ([3.9765171426981687, 1.233729640717302]),  Fake Dist ([3.901859332561493, 1.2397361161570353]) \n",
            "Epoch 1600: D (0.6910123825073242 real_err, 0.696897029876709 fake_err) G (0.6905854344367981 err); Real Dist ([3.967580376453698, 1.290626927988067]),  Fake Dist ([3.9001348555088042, 1.2300187892751102]) \n",
            "Epoch 1700: D (0.6914650201797485 real_err, 0.6954998970031738 fake_err) G (0.6916349530220032 err); Real Dist ([4.00338437128067, 1.2392375691884936]),  Fake Dist ([3.8704818074703216, 1.234384628136779]) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Btt4CO-sT2ft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adapted from https://github.com/devnag/pytorch-generative-adversarial-networks/blob/master/gan_pytorch.py"
      ]
    }
  ]
}